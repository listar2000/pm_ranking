src.pm_rank.model
=================

.. py:module:: src.pm_rank.model

.. autoapi-nested-parse::

   Model subpackage for pm_rank.



Submodules
----------

.. toctree::
   :maxdepth: 1

   /autoapi/src/pm_rank/model/average_return/index
   /autoapi/src/pm_rank/model/bradley_terry/index
   /autoapi/src/pm_rank/model/calibration/index
   /autoapi/src/pm_rank/model/irt/index
   /autoapi/src/pm_rank/model/scoring_rule/index
   /autoapi/src/pm_rank/model/utils/index


Classes
-------

.. autoapisummary::

   src.pm_rank.model.GeneralizedBT
   src.pm_rank.model.BrierScoringRule
   src.pm_rank.model.SphericalScoringRule
   src.pm_rank.model.LogScoringRule
   src.pm_rank.model.AverageReturn
   src.pm_rank.model.AverageReturnConfig
   src.pm_rank.model.CalibrationMetric


Functions
---------

.. autoapisummary::

   src.pm_rank.model.spearman_correlation
   src.pm_rank.model.kendall_correlation


Package Contents
----------------

.. py:class:: GeneralizedBT(method: Literal['MM', 'Elo'] = 'MM', num_iter: int = 100, threshold: float = 0.001, verbose: bool = False)

   Bases: :py:obj:`object`


   Generalized Bradley-Terry model for ranking forecasters in prediction markets.

   This class implements a generalization of the traditional Bradley-Terry model to
   handle prediction market scenarios. Each event outcome is treated as a contest
   between two "pseudo-teams": a winning team (the realized outcome) and a losing
   team (all other outcomes). Each forecaster contributes fractions of their capability
   proportional to their predicted probabilities.

   The model estimates skill parameters for each forecaster using an iterative
   Majorization-Minimization (MM) algorithm, which provides convergence guarantees
   and intuitive comparative scores similar to Elo ratings.

   :param method: Optimization method to use ("MM" for Majorization-Minimization).
   :param num_iter: Maximum number of iterations for the MM algorithm (default: 100).
   :param threshold: Convergence threshold for parameter updates (default: 1e-3).
   :param verbose: Whether to enable verbose logging (default: False).

   Initialize the generalized Bradley-Terry model.

   :param method: Optimization method to use ("MM" for Majorization-Minimization).
   :param num_iter: Maximum number of iterations for the MM algorithm (default: 100).
   :param threshold: Convergence threshold for parameter updates (default: 1e-3).
   :param verbose: Whether to enable verbose logging (default: False).


   .. py:attribute:: method
      :value: 'MM'



   .. py:attribute:: num_iter
      :value: 100



   .. py:attribute:: threshold
      :value: 0.001



   .. py:attribute:: verbose
      :value: False



   .. py:attribute:: logger


   .. py:method:: fit(problems: List[pm_rank.data.base.ForecastProblem], include_scores: bool = True) -> Tuple[Dict[str, Any], Dict[str, int]] | Dict[str, int]

      Fit the generalized Bradley-Terry model to the given problems.

      This method estimates skill parameters for each forecaster using the MM algorithm
      and returns rankings based on these parameters. The skill parameters represent
      the relative predictive ability of each forecaster.

      :param problems: List of ForecastProblem instances to evaluate.
      :param include_scores: Whether to include scores in the results (default: True).

      :returns: Ranking results, either as a tuple of (scores, rankings) or just rankings.



.. py:class:: BrierScoringRule(negate: bool = True, verbose: bool = False)

   Bases: :py:obj:`ScoringRule`


   Brier scoring rule for evaluating probabilistic forecasts.

   The Brier score is a quadratic proper scoring rule that measures the squared
   difference between predicted probabilities and actual outcomes. It is widely
   used in prediction markets and provides a good balance between rewarding
   accuracy and calibration.

   :param negate: Whether to negate the scores so that higher values are better
                  (default: True).
   :param verbose: Whether to enable verbose logging (default: False).

   Initialize the Brier scoring rule.

   :param negate: Whether to negate the scores so that higher values are better
                  (default: True).
   :param verbose: Whether to enable verbose logging (default: False).


   .. py:attribute:: negate
      :value: True



.. py:class:: SphericalScoringRule(verbose: bool = False)

   Bases: :py:obj:`ScoringRule`


   Spherical scoring rule for evaluating probabilistic forecasts.

   The spherical scoring rule normalizes probability vectors to unit vectors and
   measures the cosine similarity with the actual outcome. This rule is less
   sensitive to extreme probability values compared to the logarithmic rule.

   :param verbose: Whether to enable verbose logging (default: False).

   Initialize the spherical scoring rule.

   :param verbose: Whether to enable verbose logging (default: False).


.. py:class:: LogScoringRule(clip_prob: float = 0.01, verbose: bool = False)

   Bases: :py:obj:`ScoringRule`


   Logarithmic scoring rule for evaluating probabilistic forecasts.

   The logarithmic scoring rule is a proper scoring rule that rewards forecasters
   based on the logarithm of their predicted probability for the actual outcome.
   This rule heavily penalizes overconfident predictions and rewards well-calibrated
   forecasts.

   :param clip_prob: Minimum probability value to prevent log(0) (default: 0.01).
   :param verbose: Whether to enable verbose logging (default: False).

   Initialize the logarithmic scoring rule.

   :param clip_prob: Minimum probability value to prevent log(0) (default: 0.01).
   :param verbose: Whether to enable verbose logging (default: False).


   .. py:attribute:: clip_prob
      :value: 0.01



.. py:class:: AverageReturn(num_money_per_round: int = None, risk_aversion: float = None, use_approximate: bool = None, break_tie_by_uniform: bool = None, use_binary_reduction: bool = None, verbose: bool = False, config: AverageReturnConfig = None, bootstrap_ci_config: pm_rank.model.utils.BootstrapCIConfig = DEFAULT_BOOTSTRAP_CI_CONFIG)

   Average Return Model for ranking forecasters based on their expected market returns.

   This class implements a ranking algorithm that evaluates forecasters based on how much
   money they could earn from prediction markets using different risk aversion strategies.
   The model calculates expected returns for each forecaster and ranks them accordingly.

   Initialize the AverageReturn model.

   :param num_money_per_round: Amount of money to bet per round (default: 1).
   :param risk_aversion: Risk aversion parameter between 0 and 1 (default: 0.0).
   :param use_approximate: Whether to use the approximate CRRA betting strategy (default: False).
   :param break_tie_by_uniform: When the edges are all the same, 
       whether to break tie by spending uniform money on each leg. Only effective when use_approximate is True (default: True).
   :param use_binary_reduction: Whether to use the binary reduction strategy (default: False).
   :param verbose: Whether to enable verbose logging (default: False).
   :param config: Configuration object containing model parameters. If provided, individual parameters are ignored.

   :raises ValueError: If risk_aversion is not between 0 and 1.


   .. py:attribute:: num_money_per_round


   .. py:attribute:: risk_aversion


   .. py:attribute:: use_approximate


   .. py:attribute:: break_tie_by_uniform


   .. py:attribute:: use_binary_reduction


   .. py:attribute:: bootstrap_ci_config


   .. py:attribute:: verbose
      :value: False



   .. py:attribute:: logger


   .. py:attribute:: process_problem_fn


   .. py:method:: fit(problems: List[pm_rank.data.base.ForecastProblem], sharpe_mode: Literal[None, 'marginal', 'relative'] = None, include_scores: bool = True, include_bootstrap_ci: bool = False, include_per_problem_info: bool = False) -> Tuple[Dict[str, Any], Dict[str, int]] | Dict[str, int]

      Fit the average return model to the given problems.

      This method processes all problems at once and returns the final rankings
      based on average returns across all problems.

      :param problems: List of ForecastProblem instances to process.
      :param sharpe_mode: Whether to return the sharpe ratio (mean over sd). If None, we will return the average (mean) only (default: None).
          If "marginal", we will return the marginal sharpe ratio, i.e. the sharpe ratio calculated on the forecasters' earnings only.
          If "relative", we will return the relative sharpe ratio, i.e. the sharpe ratio calculated on the forecasters' earnings minus the baseline earnings.
      :param include_scores: Whether to include scores in the results (default: True).
      :param include_bootstrap_ci: Whether to include bootstrap confidence intervals in the results (default: False).
      :param include_per_problem_info: Whether to include per-problem info in the results (default: False).

      :returns: Ranking results, either as a tuple of (scores, rankings) or just rankings.
                If include_per_problem_info is True, returns a tuple of (scores, rankings, per_problem_info).



   .. py:method:: fit_stream(problem_iter: Iterator[List[pm_rank.data.base.ForecastProblem]], sharpe_mode: Literal[None, 'marginal', 'relative'] = None, include_scores: bool = True) -> Dict[int, Tuple[Dict[str, Any], Dict[str, int]] | Dict[str, int]]

      Fit the model to streaming problems and return incremental results.

      This method processes problems as they arrive and returns rankings after each batch,
      allowing for incremental analysis of forecaster performance.

      :param problem_iter: Iterator over batches of ForecastProblem instances.
      :param sharpe_mode: Whether to return the sharpe ratio (mean over sd). If None, we will return the average (mean) only (default: None).
          If "marginal", we will return the marginal sharpe ratio, i.e. the sharpe ratio calculated on the forecasters' earnings only.
          If "relative", we will return the relative sharpe ratio, i.e. the sharpe ratio calculated on the forecasters' earnings minus the baseline earnings.
      :param include_scores: Whether to include scores in the results (default: True).

      :returns: Mapping of batch indices to ranking results.



   .. py:method:: fit_stream_with_timestamp(problem_time_iter: Iterator[Tuple[str, List[pm_rank.data.base.ForecastProblem]]], sharpe_mode: Literal[None, 'marginal', 'relative'] = None, include_scores: bool = True) -> collections.OrderedDict

      Fit the model to streaming problems with timestamps and return incremental results.

      This method processes problems with associated timestamps and returns rankings
      after each batch, maintaining chronological order.

      :param problem_time_iter: Iterator over (timestamp, problems) tuples.
      :param sharpe_mode: Whether to return the sharpe ratio (mean over sd). If None, we will return the average (mean) only (default: None).
          If "marginal", we will return the marginal sharpe ratio, i.e. the sharpe ratio calculated on the forecasters' earnings only.
          If "relative", we will return the relative sharpe ratio, i.e. the sharpe ratio calculated on the forecasters' earnings minus the baseline earnings.
      :param include_scores: Whether to include scores in the results (default: True).

      :returns: Chronologically ordered mapping of timestamps to ranking results.



   .. py:method:: fit_by_category(problems: List[pm_rank.data.base.ForecastProblem], sharpe_mode: Literal[None, 'marginal', 'relative'] = None, include_scores: bool = True, stream_with_timestamp: bool = False, stream_increment_by: Literal['day', 'week', 'month'] = 'day', min_bucket_size: int = 1) -> Tuple[Dict[str, Any], Dict[str, int]] | Dict[str, int]

      Fit the average return model to the given problems by category.

      This method processes all problems at once and returns the final rankings
      based on average returns across all problems.

      :param problems: List of ForecastProblem instances to process.
      :param sharpe_mode: Whether to return the sharpe ratio (mean over sd). If None, we will return the average (mean) only (default: None).
          If "marginal", we will return the marginal sharpe ratio, i.e. the sharpe ratio calculated on the forecasters' earnings only.
          If "relative", we will return the relative sharpe ratio, i.e. the sharpe ratio calculated on the forecasters' earnings minus the baseline earnings.
      :param include_scores: Whether to include scores in the results (default: True).
      :param stream_with_timestamp: Whether to stream problems with timestamps (default: False).
      :param stream_increment_by: The increment by which to stream problems (default: "day").
      :param min_bucket_size: The minimum number of problems to include in a bucket (default: 1).



.. py:class:: AverageReturnConfig

   Configuration class for AverageReturn model parameters.

   :param num_money_per_round: Amount of money to bet per round.
   :param risk_aversion: Risk aversion parameter between 0 and 1.
       - 0: Risk neutral
       - 1: Log risk averse  
       - 0 < x < 1: Intermediate risk aversion levels
   :param use_approximate: Whether to use the approximate CRRA betting strategy.
   :param break_tie_by_uniform: When edges are all the same, whether to break tie 
       by spending uniform money on each leg. Only effective when use_approximate is True.
   :param use_binary_reduction: Whether to use the binary reduction strategy.


   .. py:attribute:: num_money_per_round
      :type:  int
      :value: 1



   .. py:attribute:: risk_aversion
      :type:  float
      :value: 0.0



   .. py:attribute:: use_approximate
      :type:  bool
      :value: False



   .. py:attribute:: break_tie_by_uniform
      :type:  bool
      :value: True



   .. py:attribute:: use_binary_reduction
      :type:  bool
      :value: False



   .. py:attribute:: bootstrap_ci_config
      :type:  pm_rank.model.utils.BootstrapCIConfig


   .. py:method:: __post_init__()

      Validate configuration parameters.



   .. py:method:: __getitem__(key)

      Allow dict-like access to config parameters.



   .. py:method:: __setitem__(key, value)

      Allow dict-like setting of config parameters.



   .. py:method:: get(key, default=None)

      Get config parameter with default value.



   .. py:method:: keys()

      Return config parameter names.



   .. py:method:: items()

      Return config parameter name-value pairs.



   .. py:method:: default() -> AverageReturnConfig
      :classmethod:


      Create a default configuration.



.. py:class:: CalibrationMetric(num_bins: int = 10, strategy: Literal['uniform', 'quantile'] = 'uniform', weight_event: bool = True, verbose: bool = False)

   
   Initialize the CalibrationMetric.

   :param num_bins: The number of bins to use for discretization.
   :param strategy: The strategy to use for discretization.
   :param weight_event: Whether to weight the event by the number of markets in it. If `False`, then each market will be treated equally.


   .. py:attribute:: num_bins
      :value: 10



   .. py:attribute:: strategy
      :value: 'uniform'



   .. py:attribute:: weight_event
      :value: True



   .. py:attribute:: verbose
      :value: False



   .. py:attribute:: logger


   .. py:method:: fit(problems: List[pm_rank.data.base.ForecastProblem], include_scores: bool = True)

      Fit the calibration metric to the given problems.

      :param problems: List of ForecastProblem instances to process.

      :returns: A dictionary containing the calibration metric.



   .. py:method:: plot(name: str, title: str = 'Reliability diagram', save_path: str = None, figsize: tuple[float, float] = (4, 4), percent: bool = True)


.. py:function:: spearman_correlation(rank_dict_a: Dict[str, int], rank_dict_b: Dict[str, int]) -> float

   Compute the Spearman correlation between two rankings.
   Reference: https://en.wikipedia.org/wiki/Spearman%27s_rank_correlation_coefficient


.. py:function:: kendall_correlation(rank_dict_a: Dict[str, int], rank_dict_b: Dict[str, int]) -> float

   Compute the Kendall correlation between two rankings.
   Reference: https://en.wikipedia.org/wiki/Kendall_rank_correlation_coefficient


