src.pm_rank
===========

.. py:module:: src.pm_rank

.. autoapi-nested-parse::

   `pm_rank`: A toolkit for scoring and ranking prediction market forecasters.



Submodules
----------

.. toctree::
   :maxdepth: 1

   /autoapi/src/pm_rank/data/index
   /autoapi/src/pm_rank/model/index


Classes
-------

.. autoapisummary::

   src.pm_rank.ForecastEvent
   src.pm_rank.ForecastProblem
   src.pm_rank.ForecastChallenge
   src.pm_rank.ChallengeLoader
   src.pm_rank.GJOChallengeLoader
   src.pm_rank.ProphetArenaChallengeLoader
   src.pm_rank.GeneralizedBT
   src.pm_rank.IRTModel
   src.pm_rank.SVIConfig
   src.pm_rank.MCMCConfig
   src.pm_rank.BrierScoringRule
   src.pm_rank.SphericalScoringRule
   src.pm_rank.AverageReturn


Functions
---------

.. autoapisummary::

   src.pm_rank.spearman_correlation
   src.pm_rank.kendall_correlation


Package Contents
----------------

.. py:class:: ForecastEvent

   Bases: :py:obj:`pydantic.BaseModel`


   Individual forecast from a user for a specific problem.


   .. py:attribute:: problem_id
      :type:  int


   .. py:attribute:: username
      :type:  str


   .. py:attribute:: timestamp
      :type:  datetime.datetime


   .. py:attribute:: probs
      :type:  List[float]


   .. py:attribute:: correct_prob
      :type:  float


   .. py:method:: validate_probabilities(v)

      Validate that probabilities sum to 1 and are non-negative.



   .. py:method:: validate_correct_probability(v, info)

      Validate that correct_prob matches one of the probabilities.



.. py:class:: ForecastProblem

   Bases: :py:obj:`pydantic.BaseModel`


   A prediction problem with multiple options and forecasts.


   .. py:attribute:: title
      :type:  str


   .. py:attribute:: problem_id
      :type:  int


   .. py:attribute:: options
      :type:  List[str]


   .. py:attribute:: correct_option
      :type:  str


   .. py:attribute:: forecasts
      :type:  List[ForecastEvent]


   .. py:attribute:: end_date
      :type:  datetime.datetime


   .. py:attribute:: num_forecasters
      :type:  int


   .. py:attribute:: url
      :type:  str | None


   .. py:attribute:: odds
      :type:  List[float] | None


   .. py:method:: validate_correct_option(v, info)

      Validate that correct_option is in the options list.



   .. py:method:: validate_forecasts(v, info)

      Validate that all forecasts have correct number of probabilities.



   .. py:method:: validate_odds(v, info)

      Validate that odds match the number of options if provided.



   .. py:property:: has_odds
      :type: bool


      Check if the problem has odds data.



   .. py:property:: crowd_probs
      :type: List[float]


      Calculate crowd probabilities from the forecasts.



   .. py:property:: unique_forecasters
      :type: List[str]


      Get list of unique forecasters for this problem.



.. py:class:: ForecastChallenge

   Bases: :py:obj:`pydantic.BaseModel`


   A collection of forecast problems with validation and computed properties.


   .. py:attribute:: title
      :type:  str


   .. py:attribute:: forecast_problems
      :type:  List[ForecastProblem]


   .. py:method:: validate_problems(v)

      Validate that there are problems and they have unique IDs.



   .. py:property:: forecaster_map
      :type: Dict[str, List[ForecastEvent]]


      Map from forecaster username to their forecasts across all problems.



   .. py:property:: num_forecasters
      :type: int


      Total number of unique forecasters across all problems.



   .. py:property:: unique_forecasters
      :type: List[str]


      List of unique forecaster usernames.



   .. py:method:: get_forecaster_problems(username: str) -> List[ForecastProblem]

      Get all problems that a specific forecaster participated in.



   .. py:method:: get_problem_by_id(problem_id: int) -> ForecastProblem | None

      Get a specific problem by its ID.



   .. py:method:: get_problems(nums: int = -1) -> List[ForecastProblem]

      Get a list of problems. If nums is -1, return all problems.



   .. py:method:: stream_problems(order: Literal['sequential', 'random', 'time'] = 'sequential', increment: int = 100) -> Iterator[List[ForecastProblem]]

      Stream the problems in the challenge. Either by random or by the problem end time.

      Args:
          order: The order in which to stream the problems.
          increment: The number of problems to stream in each iteration.

      Returns:
          An iterator of lists of problems.



   .. py:method:: stream_problems_over_time(increment_by: Literal['day', 'week', 'month'] = 'day', min_bucket_size: int = 1) -> Iterator[Tuple[str, List[ForecastProblem]]]

      Stream all problems in chronological buckets.

      Each bucket covers a contiguous time window of length *increment_by* (day, week, or
      month).  If the window does **not** yet contain *min_bucket_size* problems, the
      window is repeatedly extended by another *increment_by* until the size
      requirement is met **or** no problems remain.  All problems whose ``end_date`` is
      **strictly after** the previous bucket boundary *and* **≤** the current bucket
      boundary are included.

      The timestamp returned for a bucket is the *inclusive* upper‐bound boundary
      expressed in ISO‑8601 (YYYY‑MM‑DD).

      Args:
          increment_by: The time interval to stream problems in a bucket.
          min_bucket_size: The minimum number of problems to stream in each bucket.

      Returns:
          An iterator where each element is a bucket of (timestamp, list of problems).



   .. py:method:: fill_problem_with_fair_odds(force: bool = False) -> None

      Certain challenge do not have odds data, we can fill in fair/uniform odds for each problem.
      If `force` is True, we will not check whether the problem already has odds data.



.. py:class:: ChallengeLoader

   Bases: :py:obj:`abc.ABC`


   Abstract base class for loading forecast challenges from different data sources.
   This separates the loading logic from the data model.


   .. py:method:: load_challenge() -> ForecastChallenge
      :abstractmethod:


      Load and return a ForecastChallenge from the data source.



   .. py:method:: get_challenge_metadata() -> Dict[str, Any]
      :abstractmethod:


      Get metadata about the challenge without loading all data.



.. py:class:: GJOChallengeLoader(predictions_df: Optional[pandas.DataFrame] = None, predictions_file: Optional[str] = None, metadata_file: Optional[str] = None, challenge_title: str = '')

   Bases: :py:obj:`src.pm_rank.data.base.ChallengeLoader`


   Load forecast challenges from GJO (Good Judgment Open) data format.

   Initialize the GJOChallengeLoader. The challenge can be either loaded with a given `pd.DataFrame` or with             a combination of paths `predictions_file` and `metadata_file`.

   Args:
       predictions_df (pd.DataFrame): a pd.DataFrame containing the predictions. If provided,                 `predictions_file` and `metadata_file` will be ignored.
       predictions_file (str): the path to the predictions file
       metadata_file (str): the path to the metadata file
       challenge_title (str): the title of the challenge


   .. py:attribute:: challenge_title
      :value: ''



   .. py:attribute:: logger


   .. py:method:: load_challenge(forecaster_filter: int = 0, problem_filter: int = 0) -> src.pm_rank.data.base.ForecastChallenge

      Load challenge data from GJO format files.

      Args:
          forecaster_filter: minimum number of events for a forecaster to be included
          problem_filter: minimum number of events for a problem to be included

      Returns:
          ForecastChallenge: a ForecastChallenge object containing the forecast problems and events



   .. py:method:: get_challenge_metadata() -> Dict[str, Any]

      Get basic metadata about the GJO challenge.



.. py:class:: ProphetArenaChallengeLoader(predictions_df: Optional[pandas.DataFrame] = None, predictions_file: Optional[str] = None, challenge_title: str = '', use_bid_for_odds: bool = False)

   Bases: :py:obj:`src.pm_rank.data.base.ChallengeLoader`


   Load forecast challenges from Prophet Arena data format.

   Initialize the ProphetArenaChallengeLoader.

   The challenge can be either loaded with a given `pd.DataFrame` or with a path to a predictions file.

   :param predictions_df: A pd.DataFrame containing the predictions. If provided, `predictions_file` will be ignored.
   :param predictions_file: The path to the predictions file.
   :param challenge_title: The title of the challenge.
   :param use_bid_for_odds: Whether to use the `yes_bid` field for implied probability calculation.
                           If True, the implied probability will be calculated as the (yes_bid + no_bid) / 2.
                           If False, the implied probability will be simply `yes_ask` (normalized to sum to 1).


   .. py:attribute:: challenge_title
      :value: ''



   .. py:attribute:: use_bid_for_odds
      :value: False



   .. py:attribute:: logger


   .. py:method:: load_challenge() -> src.pm_rank.data.base.ForecastChallenge

      Load challenge data from Prophet Arena data format.
      Group by submission_id, then for each group, build the list of forecasts, then the ForecastProblem.



   .. py:method:: get_challenge_metadata() -> Dict[str, Any]

      Get basic metadata about the Prophet Arena challenge using pandas groupby (no full parsing).



.. py:class:: GeneralizedBT(method: Literal['MM', 'Elo'] = 'MM', num_iter: int = 100, threshold: float = 0.001, verbose: bool = False)

   Bases: :py:obj:`object`


   Generalized Bradley-Terry model for ranking forecasters in prediction markets.

   This class implements a generalization of the traditional Bradley-Terry model to
   handle prediction market scenarios. Each event outcome is treated as a contest
   between two "pseudo-teams": a winning team (the realized outcome) and a losing
   team (all other outcomes). Each forecaster contributes fractions of their capability
   proportional to their predicted probabilities.

   The model estimates skill parameters for each forecaster using an iterative
   Majorization-Minimization (MM) algorithm, which provides convergence guarantees
   and intuitive comparative scores similar to Elo ratings.

   :param method: Optimization method to use ("MM" for Majorization-Minimization).
   :param num_iter: Maximum number of iterations for the MM algorithm (default: 100).
   :param threshold: Convergence threshold for parameter updates (default: 1e-3).
   :param verbose: Whether to enable verbose logging (default: False).

   Initialize the generalized Bradley-Terry model.

   :param method: Optimization method to use ("MM" for Majorization-Minimization).
   :param num_iter: Maximum number of iterations for the MM algorithm (default: 100).
   :param threshold: Convergence threshold for parameter updates (default: 1e-3).
   :param verbose: Whether to enable verbose logging (default: False).


   .. py:attribute:: method
      :value: 'MM'



   .. py:attribute:: num_iter
      :value: 100



   .. py:attribute:: threshold
      :value: 0.001



   .. py:attribute:: verbose
      :value: False



   .. py:attribute:: logger


   .. py:method:: fit(problems: List[pm_rank.data.base.ForecastProblem], include_scores: bool = True) -> Tuple[Dict[str, Any], Dict[str, int]] | Dict[str, int]

      Fit the generalized Bradley-Terry model to the given problems.

      This method estimates skill parameters for each forecaster using the MM algorithm
      and returns rankings based on these parameters. The skill parameters represent
      the relative predictive ability of each forecaster.

      :param problems: List of ForecastProblem instances to evaluate.
      :param include_scores: Whether to include scores in the results (default: True).

      :returns: Ranking results, either as a tuple of (scores, rankings) or just rankings.



.. py:class:: IRTModel(n_bins: int = 6, use_empirical_quantiles: bool = False, verbose: bool = False)

   Bases: :py:obj:`object`


   Item Response Theory model for ranking forecasters using Pyro.

   This class implements an IRT model that estimates latent abilities of forecasters
   and difficulty/discrimination parameters of prediction problems. The model uses
   discretized scoring bins and supports both SVI and MCMC inference methods.

   The IRT model assumes that the probability of a forecaster achieving a certain
   score on a problem depends on their latent ability (θ), the problem's difficulty (b),
   the problem's discrimination (a), and category parameters (p) for the scoring bins.

   :param n_bins: Number of bins for discretizing continuous scores (default: 6).
   :param use_empirical_quantiles: Whether to use empirical quantiles for binning
                                  instead of uniform bins (default: False).
   :param verbose: Whether to enable verbose logging (default: False).

   Initialize the IRT model.

   :param n_bins: Number of bins for discretizing continuous scores (default: 6).
   :param use_empirical_quantiles: Whether to use empirical quantiles for binning
                                  instead of uniform bins (default: False).
   :param verbose: Whether to enable verbose logging (default: False).


   .. py:attribute:: n_bins
      :value: 6



   .. py:attribute:: use_empirical_quantiles
      :value: False



   .. py:attribute:: irt_obs
      :value: None



   .. py:attribute:: method
      :value: None



   .. py:attribute:: verbose
      :value: False



   .. py:attribute:: logger


   .. py:method:: fit(problems: List[pm_rank.data.base.ForecastProblem], include_scores: bool = True, method: Literal['SVI', 'NUTS'] = 'SVI', config: MCMCConfig | SVIConfig | None = None) -> Tuple[Dict[str, Any], Dict[str, int]] | Dict[str, int]

      Fit the IRT model to the given problems and return rankings.

      This method fits the IRT model using either SVI or MCMC inference, depending on
      the specified method. The model estimates latent abilities for each forecaster
      and difficulty/discrimination parameters for each problem.

      :param problems: List of ForecastProblem instances to fit the model to.
      :param include_scores: Whether to include scores in the results (default: True).
      :param method: Inference method to use ("SVI" for fast approximate inference
                     or "NUTS" for exact MCMC inference) (default: "SVI").
      :param config: Configuration object for the chosen inference method.
                     Must be MCMCConfig for "NUTS" or SVIConfig for "SVI".

      :returns: If include_scores=True, returns a tuple of (scores_dict, rankings_dict).
                If include_scores=False, returns only rankings_dict.
                scores_dict maps forecaster IDs to their estimated abilities.
                rankings_dict maps forecaster IDs to their ranks (1-based).

      :raises: AssertionError if method is invalid or config is not provided.



   .. py:method:: get_problem_level_parameters() -> Tuple[Dict[str, float], Dict[str, float]]

      Get problem difficulty and discrimination parameters.

      Returns the estimated difficulty and discrimination parameters for each problem
      after the model has been fitted. These parameters provide insights into how
      challenging each problem is and how well it distinguishes between forecasters
      of different abilities.

      :returns: A tuple of (difficulties_dict, discriminations_dict).
                difficulties_dict maps problem IDs to their difficulty parameters (b).
                discriminations_dict maps problem IDs to their discrimination parameters (a).

      :raises: AssertionError if the model has not been fitted yet.



.. py:class:: SVIConfig

   Bases: :py:obj:`pydantic.BaseModel`


   Configuration for SVI (Stochastic Variational Inference) optimization.

   This configuration class defines parameters for running variational inference
   using stochastic gradient descent, which provides fast approximate posterior
   inference for the IRT model.

   :param optimizer: The optimizer to use for the SVI engine ("Adam" or "SGD") (default: "Adam").
   :param num_steps: The number of steps to run for the SVI engine (default: 1000).
   :param learning_rate: The learning rate to use for the SVI engine (default: 0.01).
   :param device: The device to use for the SVI engine ("cpu" or "cuda") (default: "cpu").


   .. py:attribute:: optimizer
      :type:  Literal['Adam', 'SGD']


   .. py:attribute:: num_steps
      :type:  int


   .. py:attribute:: learning_rate
      :type:  float


   .. py:attribute:: device
      :type:  Literal['cpu', 'cuda']


.. py:class:: MCMCConfig

   Bases: :py:obj:`pydantic.BaseModel`


   Configuration for MCMC (Markov Chain Monte Carlo) inference using NUTS sampler.

   This configuration class defines parameters for running Hamiltonian Monte Carlo
   sampling with the No-U-Turn Sampler (NUTS) algorithm, which provides exact
   posterior inference for the IRT model.

   :param total_samples: The total number of samples to draw from the posterior distribution (default: 1000).
   :param warmup_steps: The number of warmup steps to run before sampling (default: 100).
   :param num_workers: The number of workers to use for parallelization. Note that we use a customized
                       multiprocessing approach since the default implementation by Pyro can be very slow.
                       This is why we don't use the name `num_chains` (default: 1).
   :param device: The device to use for the MCMC engine ("cpu" or "cuda") (default: "cpu").
   :param save_result: Whether to save the result to a file (default: False).


   .. py:attribute:: total_samples
      :type:  int


   .. py:attribute:: warmup_steps
      :type:  int


   .. py:attribute:: num_workers
      :type:  int


   .. py:attribute:: device
      :type:  Literal['cpu', 'cuda']


   .. py:attribute:: save_result
      :type:  bool


.. py:class:: BrierScoringRule(negate: bool = True, verbose: bool = False)

   Bases: :py:obj:`ScoringRule`


   Brier scoring rule for evaluating probabilistic forecasts.

   The Brier score is a quadratic proper scoring rule that measures the squared
   difference between predicted probabilities and actual outcomes. It is widely
   used in prediction markets and provides a good balance between rewarding
   accuracy and calibration.

   :param negate: Whether to negate the scores so that higher values are better
                  (default: True).
   :param verbose: Whether to enable verbose logging (default: False).

   Initialize the Brier scoring rule.

   :param negate: Whether to negate the scores so that higher values are better
                  (default: True).
   :param verbose: Whether to enable verbose logging (default: False).


   .. py:attribute:: negate
      :value: True



.. py:class:: SphericalScoringRule(verbose: bool = False)

   Bases: :py:obj:`ScoringRule`


   Spherical scoring rule for evaluating probabilistic forecasts.

   The spherical scoring rule normalizes probability vectors to unit vectors and
   measures the cosine similarity with the actual outcome. This rule is less
   sensitive to extreme probability values compared to the logarithmic rule.

   :param verbose: Whether to enable verbose logging (default: False).

   Initialize the spherical scoring rule.

   :param verbose: Whether to enable verbose logging (default: False).


.. py:class:: AverageReturn(num_money_per_round: int = 1, risk_aversion: float = 0.0, verbose: bool = False)

   Average Return Model for ranking forecasters based on their expected market returns.

   This class implements a ranking algorithm that evaluates forecasters based on how much
   money they could earn from prediction markets using different risk aversion strategies.
   The model calculates expected returns for each forecaster and ranks them accordingly.

   Initialize the AverageReturn model.

   :param num_money_per_round: Amount of money to bet per round (default: 1).
   :param risk_aversion: Risk aversion parameter between 0 and 1 (default: 0.0).
   :param verbose: Whether to enable verbose logging (default: False).

   :raises AssertionError: If risk_aversion is not between 0 and 1.


   .. py:attribute:: num_money_per_round
      :value: 1



   .. py:attribute:: risk_aversion
      :value: 0.0



   .. py:attribute:: verbose
      :value: False



   .. py:attribute:: logger


   .. py:method:: fit(problems: List[pm_rank.data.base.ForecastProblem], include_scores: bool = True) -> Tuple[Dict[str, Any], Dict[str, int]] | Dict[str, int]

      Fit the average return model to the given problems.

      This method processes all problems at once and returns the final rankings
      based on average returns across all problems.

      :param problems: List of ForecastProblem instances to process.
      :param include_scores: Whether to include scores in the results (default: True).

      :returns: Ranking results, either as a tuple of (scores, rankings) or just rankings.



   .. py:method:: fit_stream(problem_iter: Iterator[List[pm_rank.data.base.ForecastProblem]], include_scores: bool = True) -> Dict[int, Tuple[Dict[str, Any], Dict[str, int]] | Dict[str, int]]

      Fit the model to streaming problems and return incremental results.

      This method processes problems as they arrive and returns rankings after each batch,
      allowing for incremental analysis of forecaster performance.

      :param problem_iter: Iterator over batches of ForecastProblem instances.
      :param include_scores: Whether to include scores in the results (default: True).

      :returns: Mapping of batch indices to ranking results.



   .. py:method:: fit_stream_with_timestamp(problem_time_iter: Iterator[Tuple[str, List[pm_rank.data.base.ForecastProblem]]], include_scores: bool = True) -> collections.OrderedDict

      Fit the model to streaming problems with timestamps and return incremental results.

      This method processes problems with associated timestamps and returns rankings
      after each batch, maintaining chronological order.

      :param problem_time_iter: Iterator over (timestamp, problems) tuples.
      :param include_scores: Whether to include scores in the results (default: True).

      :returns: Chronologically ordered mapping of timestamps to ranking results.



.. py:function:: spearman_correlation(rank_dict_a: Dict[str, int], rank_dict_b: Dict[str, int]) -> float

   Compute the Spearman correlation between two rankings.
   Reference: https://en.wikipedia.org/wiki/Spearman%27s_rank_correlation_coefficient


.. py:function:: kendall_correlation(rank_dict_a: Dict[str, int], rank_dict_b: Dict[str, int]) -> float

   Compute the Kendall correlation between two rankings.
   Reference: https://en.wikipedia.org/wiki/Kendall_rank_correlation_coefficient


